###需要先看一下 SDL 播放音频数据的基础
* 参考： [http://blog.csdn.net/leixiaohua1020/article/details/40544521](http://blog.csdn.net/leixiaohua1020/article/details/40544521)
* 这个源码里也要加上（在所有include之后） #undef main
* 简化（去掉了循环播放，用 memcpy 代替了 SDLMix ）后的代码如下：

		#include <SDL.h>
		#include <SDL_thread.h>
		
		#include <stdio.h>
		#include <tchar.h>
		
		#undef main 
		
		static  Uint8  *audio_chunk;
		static  Uint32  audio_len;
		static  Uint8  *audio_pos;
		
		/* Audio Callback
		* The audio function callback takes the following parameters:
		* stream: A pointer to the audio buffer to be filled
		* len: The length (in bytes) of the audio buffer
		*
		*/
		void  fill_audio(void *udata, Uint8 *stream, int len) {
			if (audio_len == 0)
				return;
			len = (len>audio_len ? audio_len : len);
		
			memcpy(stream, audio_pos, len);
			audio_pos += len;
			audio_len -= len;
		}
		
		int main(int argc, char* argv[])
		{
			//Init
			if (SDL_Init(SDL_INIT_AUDIO | SDL_INIT_TIMER)) {
				printf("Could not initialize SDL - %s\n", SDL_GetError());
				return -1;
			}
			//SDL_AudioSpec
			SDL_AudioSpec wanted_spec;
			wanted_spec.freq = 44100;
			wanted_spec.format = AUDIO_S16SYS;
			wanted_spec.channels = 2;
			wanted_spec.silence = 0;
			wanted_spec.samples = 1024;
			wanted_spec.callback = fill_audio;
		
			if (SDL_OpenAudio(&wanted_spec, NULL)<0) {
				printf("can't open audio.\n");
				return -1;
			}
		
			FILE *fp = fopen("D:\\CppWorkSpace\\FFmpegTutorial\\x64\\Debug\\chimei_416x240_42sec.pcm", "rb+");
			if (fp == NULL) {
				printf("cannot open this file\n");
				return -1;
			}
			int pcm_buffer_size = 4096;
			char *pcm_buffer = (char *)malloc(pcm_buffer_size);
		
			//Play
			SDL_PauseAudio(0);
		
			while (fread(pcm_buffer, 1, pcm_buffer_size, fp)) {
				//Set audio buffer (PCM data)
				audio_chunk = (Uint8 *)pcm_buffer;
				//Audio buffer length
				audio_len = pcm_buffer_size;
				audio_pos = audio_chunk;
				while (audio_len>0)//Wait until finish
					SDL_Delay(1);
			}
			free(pcm_buffer);
			SDL_Quit();
		
			return 0;
		}

* 注：代码中用到的pcm音频文件可以用ffmpeg.exe生成：`ffmpeg -i input.mp4 -vn -f s16le -ar 44100 -acodec pcm_s16le output.pcm`

#### SDL 播放音频
* `SDL_OpenAudio()` 会打开音频设备，打开之后并没有开始播放
* `SDL_PauseAudio()` 会让音频设备开始播放音频 
* 音频设备播放音频： 
	* 当音频设备觉得该播放下一段了，会调用一个回调函数，让回调函数送数据过来。所以回调函数有个参数是音频设备缓冲区的地址，让回调函数往里写音频数据的，当然还有一个参数告诉你我需要多大的数据，你别写多了。
	* 所以回调函数就干一件事儿就行了：给缓冲区写数据。比如用 `memcpy()`。
	* 音频设备播放缓冲区里的数据时，是按照打开音频设备时（`SDL_OpenAudio()`）设定的参数播的：比如你设定了采样率是 44100，那么它就每秒播放 44100 个采样。比如你设定了采样格式是“signed 16bit 小端存储”，那么音频设备就按每个采样16bit去读取并播放。
	* 当音频设备觉得缓冲区的数据播完了的时候，它自然就去调用回调函数要数据了。
	* 另外回调函数还有一个参数用来存放用户信息。

### Tutorial03

#### 题外话
* 为了产生一个 tutorial3 代码能正常播放声音的文件，必须生成音频的 `sample_fmt` 是 s16 的
* `ffmpeg -sample_fmts` 可以查看所有支持的采样格式
* 尝试尽了各种组合，比如： `ffmpeg -i input.mp4 -vcodec h264 -ar 44100 -acodec mp2 -sample_fmt s16 out.mp4`
* 但要么转不出来，要么转出来的文件音频格式不是 s16（用 ffprobe 看，上述命令的音频采样格式是 s16p，多了个p，差别千里）

#### 修改代码以便能运行

* 改了一行报错的代码，加了强转：

		pkt1 = (AVPacketList *)av_malloc(sizeof(AVPacketList));

* 音频的基本知识看 tutorial 的讲解就可以了

#### PCM 的格式

* 代码里有这么一句

		wanted_spec.format = AUDIO_S16SYS;

* tutorial 的解释是： `This is the format that avcodec_decode_audio2 will give us the audio in.`
* 但是查 `avcodec_decode_audio2`（其实是 `avcodec_decode_audio4`）的文档没有说解码完的音频是什么格式
* 其实解码成什么格式，是从 CodecContext 里读到的，（跟解码视频一样，视频的 `pix_fmt` 从 CodecContext 里读来)
* 就是说，对音视频进行编码的时候，就说好了解码出来是什么格式的音/视频
* 视频格式有 yuv420p，yuv422 等等很多种，音频也一样。 音频有 `unsigned 8 bits`，`signed 16 bits, planar` 等等。
* 音频未压缩的原始数据是以‘采样（Sample）’为单元的，就类似于图像数据的帧。`unsigned 8 bits` 就表示一个采样用 8 bit 无符号数据来表示。
* ffmpeg 的音频采样格式封装在枚举`enum AVSampleFormat`里
* 在 `AVCodecContext` 里，用字段 `enum AVSampleFormat sample_fmt` 来表示音频数据的采样格式。
* 所以以上代码，精确的写法应该类似这样：

		switch (aCodecCtx->sample_fmt)
		{
		case AV_SAMPLE_FMT_S16:
			wanted_spec.format = AUDIO_S16SYS;
			break;
		case AV_SAMPLE_FMT_U8:
			wanted_spec.format = AUDIO_U8;
			break;
		default:
			wanted_spec.format = AUDIO_S16SYS;
			break;
		}

* SDL 和 ffmpeg 对原始音频数据的封装还是有很大区别：ffmpeg 里定义了 8 bit，16 bit，32 bit，有无符号，浮点型，Double 型，是否 Planar 存储等10多种音频采样格式，而 SDL 只定义了 8 bit，16 bit，有无符号，大小端存储等6种。
	* ffmpeg 不区分大小端存储，统一使用跟系统相同的字节序：`The data described by the sample format is always in native-endian order.`
	* SDL 可以手动区分大小端存储：`AUDIO_U16LSB` 16 bit unsigned 小端存储。`AUDIO_S16MSB`16 bit unsigned 大端存储。
	* SDL 用后缀 LSB 表示小端存储，MSB 表示大端存储，SYS 表示跟系统一致（native byte ordering）
* 总之要把 ffmpeg 解码出来的原始音频数据 map 到 SDL 能处理的格式。 视频也是一样的。
* 不过基于以上说的，他俩表示音频格式的方法差别较大，所以几乎没办法一一对应的 map。能一一对应的，也就上面代码里的那两种。
* 如果（现实中应该是经常）遇到 ffmpeg 解出来而 SDL 不支持的格式，我们还要想别的办法，比如重采样（类似于视频的 swscale，本文后续会讲）什么的。
* 因为 tutorial03 的代码，强制把 `wanted_spec.format` 设置成了 `AUDIO_S16SYS`，所以 SDL 在播放音频的时候，就按照 signed 16 bit native-order 来播。
* 但是，如果你打开的音频不是这种格式的，你就会听到奇怪的声音。比如我测试的时候从网上下载了个mp4，用ffprobe看的时候，其音频流的采样格式为 fltp，是一种 32 bit 浮点数表示法，不是 signed 16。
* 所以用 tutorial03 播起来声音全是噪声。

#### 视频会飞快播完，声音正常播放
* tutorial03 的代码播放一个文件，视频会飞快播完，声音正常播放
* 这是 SDL 播放音频和播放视频的机制不同引起的
* 播放视频，是我们主动让它播，我们每解一帧就让他播一帧，所以播的飞快，因为解码飞快
* 而音频，SDL 是自己播的，他按照我们打开音频设备时设置好的采样率、采样格式去播，所以它知道一秒钟该播放多少个采样。当它播完了缓冲区中的所有采样，它主动调回调函数找我们要数据。
* 所以SDL播音频听起来是正常速度。
* 由于视频播完程序就退出了，所以正常速度播放的声音可能没听全就结束了。可以在读packet那个while结束的地方加个 getchar()，这样就能听全整首歌了。

#### resample
* 类似于 swscale，对于音频，我们要先 resample，以便把解码出来的原始音频格式转换成SDL能播放原始音频格式。
* 网上找了个代码：[http://blog.csdn.net/zhuweigangzwg/article/details/43733673](http://blog.csdn.net/zhuweigangzwg/article/details/43733673)
* 这段代码是个函数，只需要给出resample之前的CodecContext和frame，以及想要resample成的采样格式，声道，采样频率，以及resample后的数据地址，即可以进行重采样。比自己写方便多了。
* 整个函数的代码如下：

		int AudioResampling(AVCodecContext * audio_dec_ctx, AVFrame * pAudioDecodeFrame,
			int out_sample_fmt, int out_channels, int out_sample_rate, uint8_t * out_buf)
		{
			//////////////////////////////////////////////////////////////////////////
			SwrContext * swr_ctx = NULL;
			int data_size = 0;
			int ret = 0;
			int64_t src_ch_layout = AV_CH_LAYOUT_STEREO; //初始化这样根据不同文件做调整
			int64_t dst_ch_layout = AV_CH_LAYOUT_STEREO; //这里设定ok
			int dst_nb_channels = 0;
			int dst_linesize = 0;
			int src_nb_samples = 0;
			int dst_nb_samples = 0;
			int max_dst_nb_samples = 0;
			uint8_t **dst_data = NULL;
			int resampled_data_size = 0;
		
			//重新采样
			if (swr_ctx)
			{
				swr_free(&swr_ctx);
			}
			swr_ctx = swr_alloc();
			if (!swr_ctx)
			{
				printf("swr_alloc error \n");
				return -1;
			}
		
			src_ch_layout = (audio_dec_ctx->channel_layout &&
				audio_dec_ctx->channels ==
				av_get_channel_layout_nb_channels(audio_dec_ctx->channel_layout)) ?
				audio_dec_ctx->channel_layout :
				av_get_default_channel_layout(audio_dec_ctx->channels);
		
			if (out_channels == 1)
			{
				dst_ch_layout = AV_CH_LAYOUT_MONO;
			}
			else if (out_channels == 2)
			{
				dst_ch_layout = AV_CH_LAYOUT_STEREO;
			}
			else
			{
				//可扩展
			}
		
			if (src_ch_layout <= 0)
			{
				printf("src_ch_layout error \n");
				return -1;
			}
		
			src_nb_samples = pAudioDecodeFrame->nb_samples;
			if (src_nb_samples <= 0)
			{
				printf("src_nb_samples error \n");
				return -1;
			}
		
			/* set options */
			av_opt_set_int(swr_ctx, "in_channel_layout", src_ch_layout, 0);
			av_opt_set_int(swr_ctx, "in_sample_rate", audio_dec_ctx->sample_rate, 0);
			av_opt_set_sample_fmt(swr_ctx, "in_sample_fmt", audio_dec_ctx->sample_fmt, 0);
		
			av_opt_set_int(swr_ctx, "out_channel_layout", dst_ch_layout, 0);
			av_opt_set_int(swr_ctx, "out_sample_rate", out_sample_rate, 0);
			av_opt_set_sample_fmt(swr_ctx, "out_sample_fmt", (AVSampleFormat)out_sample_fmt, 0);
			swr_init(swr_ctx);
		
			max_dst_nb_samples = dst_nb_samples =
				av_rescale_rnd(src_nb_samples, out_sample_rate, audio_dec_ctx->sample_rate, AV_ROUND_UP);
			if (max_dst_nb_samples <= 0)
			{
				printf("av_rescale_rnd error \n");
				return -1;
			}
		
			dst_nb_channels = av_get_channel_layout_nb_channels(dst_ch_layout);
			ret = av_samples_alloc_array_and_samples(&dst_data, &dst_linesize, dst_nb_channels,
				dst_nb_samples, (AVSampleFormat)out_sample_fmt, 0);
			if (ret < 0)
			{
				printf("av_samples_alloc_array_and_samples error \n");
				return -1;
			}
		
		
			dst_nb_samples = av_rescale_rnd(swr_get_delay(swr_ctx, audio_dec_ctx->sample_rate) +
				src_nb_samples, out_sample_rate, audio_dec_ctx->sample_rate, AV_ROUND_UP);
			if (dst_nb_samples <= 0)
			{
				printf("av_rescale_rnd error \n");
				return -1;
			}
			if (dst_nb_samples > max_dst_nb_samples)
			{
				av_free(dst_data[0]);
				ret = av_samples_alloc(dst_data, &dst_linesize, dst_nb_channels,
					dst_nb_samples, (AVSampleFormat)out_sample_fmt, 1);
				max_dst_nb_samples = dst_nb_samples;
			}
		
			data_size = av_samples_get_buffer_size(NULL, audio_dec_ctx->channels,
				pAudioDecodeFrame->nb_samples,
				audio_dec_ctx->sample_fmt, 1);
			if (data_size <= 0)
			{
				printf("av_samples_get_buffer_size error \n");
				return -1;
			}
			resampled_data_size = data_size;
		
			if (swr_ctx)
			{
				ret = swr_convert(swr_ctx, dst_data, dst_nb_samples,
					(const uint8_t **)pAudioDecodeFrame->data, pAudioDecodeFrame->nb_samples);
				if (ret <= 0)
				{
					printf("swr_convert error \n");
					return -1;
				}
		
				resampled_data_size = av_samples_get_buffer_size(&dst_linesize, dst_nb_channels,
					ret, (AVSampleFormat)out_sample_fmt, 1);
				if (resampled_data_size <= 0)
				{
					printf("av_samples_get_buffer_size error \n");
					return -1;
				}
			}
			else
			{
				printf("swr_ctx null error \n");
				return -1;
			}
		
			//将值返回去
			memcpy(out_buf, dst_data[0], resampled_data_size);
		
			if (dst_data)
			{
				av_freep(&dst_data[0]);
			}
			av_freep(&dst_data);
			dst_data = NULL;
		
			if (swr_ctx)
			{
				swr_free(&swr_ctx);
			}
			return resampled_data_size;
		}

* 使用这段代码要引入一下头文件：

		#include <libswresample/swresample.h>
		#include <libavutil/opt.h>

* 在 tutorial03 的代码里，每次解码完一帧，都把这一帧进行resample，修改的代码如下：

		if (got_frame) {
			//data_size = av_samples_get_buffer_size(NULL,
			//	aCodecCtx->channels,
			//	frame.nb_samples,
			//	aCodecCtx->sample_fmt,
			//	1);
			//assert(data_size <= buf_size);
			data_size = AudioResampling(aCodecCtx,&frame,MY_AUDIO_SAMPLE_FMT_FFMPEG,MY_AUDIO_CHANNEL,MY_AUDIO_SMAPLE_RATE,audio_buf);
			//memcpy(audio_buf, frame.data[0], data_size);
		}

* 直接把 resample 后的数据拷贝给 `audio_decode_frame` 函数的输出参数 `audio_buf`
* `AudioResampling` 函数返回值的返回值就是从采样生成了多少字节数据，我们正好把这个数据赋值给 `data_size` 就可以了。
* 经过重采样，就可以播放音频采样格式不是 u16 的了，我播了一个 fltp 的，声音是对的，不过听起来有噪音。
* 估计是因为重采样，用一半的数据量来表示一个 sample，肯定造成了音质损失。
* 具体怎么解决这个问题，以后再搞。

#### 不用 resample
* 只需要把存储的数据转换一下即可，重采样太耗时了，而且容易出问题。
* 把 s16p 转换成 s16 即可，因为市面上还是有挺多 s16p 的音频的，转成 s16 SDL 就能播了。
* s16p 和 s16 都是用 signed 16 bit 表示一个 sample，所以基本不需要运算就能完成转换，只需看一下 ffmpeg 分别是怎么存储这两种 sample fmt 的即可
* 根据 `enum AVSampleFormat` 的注释：

		 * For planar sample formats, each audio channel is in a separate data plane,
		 * and linesize is the buffer size, in bytes, for a single plane. All data
		 * planes must be the same size. For packed sample formats, only the first data
		 * plane is used, and samples for each channel are interleaved. In this case,
		 * linesize is the buffer size, in bytes, for the 1 plane.

* 我们待转的格式是 s16p，即 Planar 格式的，如果是 2 声道，则 data[0] 和 data[1] 分别存放两个声道的数据，每个 sample 16bit
* 我们的目标格式是 s16， 即 Packed 格式的，2个声道的数据都要存放在 data[0] 里

* 参考：[http://my.os
* china.net/u/589963/blog/168301](http://my.oschina.net/u/589963/blog/168301)
* 这里面有一处注释应该是写错了：

		//short==2bit, 2 channel

* 应该是 2bytes，而不是2bit。 因为目标格式是 s16p，所以一个 sample 应该占用 16bit 即 2byte 
* 我们要做的实际上比这个还简单，我们只需要把 s16p 转成 s16 即可，不涉及 float 转 signed

#### 应该是 ffmpeg 编码时的一个 bug
* `ffmpeg -i chimei_416x240_42sec.mp4 -acodec mp2 -ac 2 -ar 44100 -sample_fmt s16 -vcodec h264  out.mp4`
* 以上命令编码，按理说应该产生的是 `sample_fmt` 为 s16 的音频
* 但是，用 ffprobe 看，结果却是：

    	Stream #0:1(eng): Audio: mp3 (mp4a / 0x6134706D), 44100 Hz, stereo, s16p, 383 kb/s (default)

* 里面的 s16p 代表 `sample_fmt`，说明 ffmpeg 编码的时候，在封装层把 `sample_fmt` 给写错了
* 当然，你也可以认为是 ffprobe 错了，所以接着证明是 ffmpeg 错的：
* 首先，用代码读取pkt，看 `aCodecContext->sample_fmt` ，发现等于6，即 `AV_SAMPLE_FMT_S16P` （而不是1，`AV_SAMPLE_FMT_S16`）
* 然后，看解码出来的 frame，其中的 `frame.linesize[0] `永远等于 `frame.nb_samples*2`，乘以的2是因为16bit一个sample，即2字节一个sample。也就是说，所有的sample都在data[0]里，这不符合 planar 存储方式。
* 说明 ffmpeg 在转码的时候，是按照 s16 存储的（而不是s16p），这是对的，这符合命令行参数。但是错在，在mux的时候，把 `sample_fmt` 字段给写错了，写成了 s16p。